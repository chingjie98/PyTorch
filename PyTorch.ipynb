{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92fa59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d477da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2801, -0.8547, -0.3221], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f36fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2801, 1.1453, 1.6779], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74812ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7011, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y + 3\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe94f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()  #dz / dx  --> simply to calculate the gradients by calling backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "058376e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 0.3333, 0.3333])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72131183",
   "metadata": {},
   "source": [
    "<h3> Example of a training loop </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acfda317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad = True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights * 3).sum()\n",
    "    \n",
    "    model_output.backward()     #calculate gradients with respect to the weights.\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    weights.grad.zero_()        #prevent gradients from being tracked. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19d25d",
   "metadata": {},
   "source": [
    "<h3> Example on 1 forward and backward propagation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab9a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "weight = torch.tensor(1.0, requires_grad = True)\n",
    "x = torch.tensor(1.0)     #fixed\n",
    "y = torch.tensor(2.0)     #fixed\n",
    "\n",
    "y_hat = weight * x \n",
    "\n",
    "loss = (y_hat - y) ** 2       #loss function\n",
    "\n",
    "loss.backward()          #gradient of the loss function with respect to weight\n",
    "print(weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99498e",
   "metadata": {},
   "source": [
    "<h3> From Scratch using Numpy - Linear Regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a5044ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314574\n",
      "epoch 7: w = 1.997, loss = 0.00050331\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 12: w = 2.000, loss = 0.00000005\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "Y = np.array([2,4,6,8], dtype = np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "#loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "\n",
    "#gradient\n",
    "#MSE = 1/N * (w*x - y)**2\n",
    "#dJ/dw = 1/N 2x (w*x - y)\n",
    "\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(X,Y,y_pred)\n",
    "    \n",
    "    #update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3e33c",
   "metadata": {},
   "source": [
    "<h3> Using Pytorch </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d2fca3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n",
      "\n",
      "Not as good as numpy because the backward propagation is not exact like the numpy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "#model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "#loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
    "print('\\nNot as good as numpy because the backward propagation is not exact like the numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c0fcf",
   "metadata": {},
   "source": [
    "### Training Pipeline: Model, Loss and Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e99774f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 4.893\n",
      "epoch 1: w = 0.920, loss = 6.13462734\n",
      "epoch 11: w = 1.488, loss = 0.39158028\n",
      "epoch 21: w = 1.589, loss = 0.22944006\n",
      "epoch 31: w = 1.615, loss = 0.21248014\n",
      "epoch 41: w = 1.628, loss = 0.20001943\n",
      "epoch 51: w = 1.640, loss = 0.18837476\n",
      "epoch 61: w = 1.650, loss = 0.17741027\n",
      "epoch 71: w = 1.661, loss = 0.16708417\n",
      "epoch 81: w = 1.671, loss = 0.15735891\n",
      "epoch 91: w = 1.681, loss = 0.14819986\n",
      "Prediction after training: f(5) = 9.360\n",
      "\n",
      "Not as good as numpy because the backward propagation is not exact like the numpy\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input size, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#    - forward pass: compute prediction\n",
    "#    - backward pass: gradients\n",
    "#    - update weights\n",
    "\n",
    "\n",
    "#Basically with PyTorch, we only need to know which \n",
    "#Loss and Optimizer we need to use.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "#don't need weights and forward fn anymore since PyTorch will help. \n",
    "#need to initialize a 2d array where the number of rows is the number of samples\n",
    "\n",
    "#([4,1])\n",
    "n_samples, n_features = X.shape  \n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# model = nn.Linear(input_size, output_size)       # 1 layer\n",
    "\n",
    "#Design Model --------------------------------------------------------------\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)    \n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "\n",
    "#Construct Loss and Optimizer -----------------------------------------------\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "#Training Loop --------------------------------------------------------------\n",
    "\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "        \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')\n",
    "print('\\nNot as good as numpy because the backward propagation is not exact like the numpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2cdef",
   "metadata": {},
   "source": [
    "### Linear Regression with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a44d3019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 4441.8330\n",
      "epoch: 20, loss = 3312.5134\n",
      "epoch: 30, loss = 2495.5005\n",
      "epoch: 40, loss = 1903.7920\n",
      "epoch: 50, loss = 1474.8286\n",
      "epoch: 60, loss = 1163.5613\n",
      "epoch: 70, loss = 937.5048\n",
      "epoch: 80, loss = 773.2037\n",
      "epoch: 90, loss = 653.7011\n",
      "epoch: 100, loss = 566.7242\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#0) prepare data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1) #reshape tensor\n",
    "\n",
    "n_samples, n_features = X.shape  #([100,1])\n",
    "\n",
    "#1) model\n",
    "input_size = n_features\n",
    "output_size = 1        #each sample that we put in, our model gets 1 output.\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "#2) loss and optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    #forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    #update\n",
    "    optimizer.step()\n",
    "    \n",
    "    #empty gradients because whenever we call the backward, it will sum up the gradients into the dot product attribute\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss = {loss.item():.4f}')\n",
    "    \n",
    "#plot\n",
    "predicted = model(X).detach().numpy() #detach to remove the require gradient in Tensor so that we can convert into numpy.\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9453a5",
   "metadata": {},
   "source": [
    "### Logistic Regression using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e39486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss = 0.4738\n",
      "epoch: 20, loss = 0.4022\n",
      "epoch: 30, loss = 0.3542\n",
      "epoch: 40, loss = 0.3198\n",
      "epoch: 50, loss = 0.2940\n",
      "epoch: 60, loss = 0.2738\n",
      "epoch: 70, loss = 0.2575\n",
      "epoch: 80, loss = 0.2440\n",
      "epoch: 90, loss = 0.2327\n",
      "epoch: 100, loss = 0.2229\n",
      "accuracy = 0.8947\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input size, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#    - forward pass: compute prediction\n",
    "#    - backward pass: gradients\n",
    "#    - update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets   # to load binary classification dataset.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#0) prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X,y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape     #([569, 30])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "#scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)   #put each value in 1 row, with total only 1 column.\n",
    "y_test = y_test.view(y_test.shape[0], 1)   \n",
    "\n",
    "\n",
    "#1) model\n",
    "# f = wx + b, sigmoid at the end\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)   #first we apply the linear layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))    #then, we apply the sigmoid function\n",
    "        return y_predicted\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "\n",
    "#2) loss and optimizer\n",
    "criterion = nn.BCELoss() #binary cross entrophy loss\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "#3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #forward pass and loss calculation\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "    #backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    #updates\n",
    "    optimizer.step()\n",
    "    \n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "        \n",
    "with torch.no_grad():      #Evaluation should not be part of our computational graph where we want to track the history, so we remove the gradient tracking.\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_classes = y_predicted.round() #those above 0.5 turn to 1. somemore, if we don't use the no_grad above, we cannot do this, since got require_grad.\n",
    "    acc = y_predicted_classes.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e41369",
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
